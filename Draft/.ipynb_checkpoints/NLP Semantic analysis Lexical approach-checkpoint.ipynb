{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/etd/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/etd/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/etd/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /home/etd/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /home/etd/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /home/etd/nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.\tLes bases de la NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1.\tSegmentation (Tokenization)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La segmentation de texte et la tâche de subdivision du texte en petites unités qui seront plus simples à traiter et qu’on appelle tokens.\n",
    "La bibliothèque nltk offre à travers le module tekenize un certain nombre de tokinzers qui permettent de réaliser la segmentation du texte en fonction de la nature du problème : words tokenizer, regular-expression based tokenizer, sentences based tokinizers, etc. Ci-dessous une liste non exhaustive de quelques fonctions du module tokinize. \n",
    "\n",
    "-regexp_span_tokenize(text, regexp: Retourne les tokens de texte qui correspondent à l’expression régulière regexp\n",
    "-sent_tokenize(text[, language]):\tRetourn les phrases contenues dans le texte en utilisant le tokenizer PunktSentenceTokenizer.\n",
    "-word_tokenize(text[, language]:\tRetourn les mots contenus dans le texte en utilisant le tokenizer TreebankWordTokenizer avec PunktSentenceTokenizer. \n",
    "\n",
    "nltk offre également un certain nombre de classes qui offrent des tokinizers plus avancés : BlanklineTokenizer, MWETokenizer, PunktSentenceTokenizer, TextTilingTokenizer, TweetTokenizer, etc. \n",
    "Ci-dessous deux exemples de tokenization à base de sent_tokenize et word_tokenize:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Hello, i am very happy to meet you.', 'I created this course for you.', 'Good by!']\n",
      "['Hello', ',', 'i', 'am', 'very', 'happy', 'to', 'meet', 'you', '.', 'I', 'created', 'this', 'course', 'for', 'you', '.', 'Good', 'by', '!']\n"
     ]
    }
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "\n",
    "sentences=sent_tokenize(data) #divise en phrase\n",
    "print(sentences)\n",
    "words=word_tokenize(data)#divise en word token\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2.\tNettoyage(Cleaning) \n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le nettoyage des données texte joue un rôle très important dans l’amélioration des performances des opérations d’analyse et de découverte de paternes. Ça consiste à la suppression des termes non significatifs \"Stop words\", comme par exemple « le », « la », « de », « du », « ce »… en français et « as » « the », « a », « an », « in » en anglais.  Ces termes qui sont présents fréquemment dans des documents texte peuvent influencer négativement sur la qualité des résultats d’analyse. \n",
    "Le nettoyage peut consister également à la supression des caractères de pontuation et des chaînes de caractères non alphabétiques. \n",
    "Ci-dessous le code qui permet de supprimer les stop words à partir d’un texte."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'happy', 'meet', 'created', 'course', 'good']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"Hello, i am very happy to meet you. I created this course for you. Good by!\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "data_clean = [word for word in word_tokens if (not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(data_clean)\n",
    "#un mot vide (ou stop word, en anglais) est un mot qui est tellement commun qu'il est inutile de l'indexer ou de l'utiliser dans une recherche. En français, des mots vides évidents pourraient être « le », « la », « de », « du », « ce »…"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3.\tRacinisation(Stemming)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La racinisation permet de normaliser la représentation des mots contenus dans une expression texte en extrayant leurs racines. Ça permettra de supprimer toutes les redondances des mots ayant la même racine. Plusieurs stemmers sont offerts par nltk dont les plus utilisés sont : PorterStemmer, LancasterStemmer et SnowballStemmer. Également, le module nltk.stem.snowball offre un certain nombre de stemmers personnalisés à chaque langue, comme par exemple :  FrenchStemmer, ArabicStemmer, etc. \n",
    "\n",
    "\n",
    "\n",
    "#La racine d'un mot correspond à la partie du mot restante une fois que l'on a supprimé son (ses) préfixe(s) et suffixe(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'gross', 'donné', 'anglai', 'mégadonné', 'donné', 'massiv', 'désign', 'ressourc', 'inform', 'dont', 'caractéristiqu', 'term', 'volum', 'vélocité', 'variété', 'impos', 'utilis', 'technolog', 'méthode', 'analytiqu', 'particulièr', 'générer', 'valeur', 'dépassent', 'général', 'capacité', 'seul', 'uniqu', 'machin', 'nécessit', 'traitement', 'parallélisé']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "#stemmer=SnowballStemmer('french')\n",
    "stemmer=PorterStemmer()\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "for i in range(len(word_tokens)):\n",
    "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('french')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'dat', 'gross', 'don', 'anglais', 'mégadon', 'don', 'massiv', 'désign', 'ressourc', 'inform', 'dont', 'caractérist', 'term', 'volum', 'véloc', 'variet', 'imposent', 'utilis', 'technolog', 'méthod', 'analyt', 'particuli', 'géner', 'valeur', 'dep', 'général', 'capac', 'seul', 'uniqu', 'machin', 'nécessitent', 'trait', 'parallélis']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer,SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stemmer=SnowballStemmer('french')\n",
    "#stemmer=PorterStemmer()\n",
    "data=\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\"\n",
    "word_tokens = [word.lower() for word in word_tokenize(data)]\n",
    "\n",
    "for i in range(len(words)):\n",
    "    words=[stemmer.stem(word) for word in word_tokens if (not word in set(stopwords.words('french')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.4.\tLemmatisation"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A la différence de la racisation qui fournit souvent une représentation non significative et incomplète des mots, la lemmatisation permet d’obtenir les formes canoniques des mots contenus dans une expression texte. Ainsi, au lieu de supprimer juste les suffixes et les préfixes des mots pour obtenir leurs racines, la lemmatisation réalise une analyse morphologique des mots afin d’extraire leurs formats canoniques.\n",
    "nltk offre le lemmatizer  WordNetLemmatizer pour la réalisation des opérations de lemmatisation, mais uniquement pour l’anglais. pour d'autre langues voir Spacy dans la section 3.8.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['big', 'data', 'field', 'treat', 'way', 'analyze', 'systematically', 'extract', 'information', 'otherwise', 'deal', 'data', 'set', 'large', 'complex', 'dealt', 'traditional', 'application', 'software', 'data', 'many', 'case', 'row', 'offer', 'greater', 'statistical', 'power', 'data', 'higher', 'complexity', 'attribute', 'column', 'may', 'lead', 'higher', 'false', 'discovery', 'rate']\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "lemmmatizer=WordNetLemmatizer()\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "words = word_tokenize(data)\n",
    "words = [lemmmatizer.lemmatize(word.lower()) for word in words if(not word in set(stopwords.words('english')) and  word.isalpha())]\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5.\tPOS-Tagging"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Le pos-tagging permet des réaliser une analyse lexicale d’une expression texte selon les règles de la grammaire. Les différentes unités seront dotées d’une annotation permettant de savoir le rôle grammatical de chaque mot dans l’expression. Les annotations les plus courante sont (DT : Determiner, NN : noun , JJ : adjective,  RB: adverb, VB : verb,  PRP : Personal Pronoun…).\n",
    "\n",
    "NLTK offre une panoplie de taggers pour le pos-taggin qui recoivent une liste de tokens et leurs attribuent automatiquement  des tags en se basant sur des corpus d'apprentisgae.  \n",
    "\n",
    "par defaut la methode pos_tag offre un pos_tagging standard (Recommendé) pour l'anglais et cela en se basant sur le tagset \"Penn Treebank\":c\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Big', 'NNP'), ('data', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('that', 'WDT'), ('treats', 'VBZ'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'VB'), (',', ','), ('systematically', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN'), ('.', '.'), ('Data', 'NNP'), ('with', 'IN'), ('many', 'JJ'), ('cases', 'NNS'), ('(', '('), ('rows', 'NNS'), (')', ')'), ('offer', 'VBP'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'IN'), ('data', 'NNS'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'RBR'), ('attributes', 'NNS'), ('or', 'CC'), ('columns', 'NNS'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'DT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "words=word_tokenize(data)\n",
    "print(nltk.pos_tag(words))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Dans le cas d'un document qui se compose de plusieurs phrases, il sera preferable d'utliser pos_tag_sents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[('Big', 'NNP'), ('data', 'NN'), ('is', 'VBZ'), ('a', 'DT'), ('field', 'NN'), ('that', 'WDT'), ('treats', 'VBZ'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'VB'), (',', ','), ('systematically', 'RB'), ('extract', 'JJ'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'NN'), ('with', 'IN'), ('data', 'NNS'), ('sets', 'NNS'), ('that', 'WDT'), ('are', 'VBP'), ('too', 'RB'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'VB'), ('dealt', 'VBN'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'JJ'), ('application', 'NN'), ('software', 'NN'), ('.', '.')], [('Data', 'NNP'), ('with', 'IN'), ('many', 'JJ'), ('cases', 'NNS'), ('(', '('), ('rows', 'NNS'), (')', ')'), ('offer', 'VBP'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'IN'), ('data', 'NNS'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'RBR'), ('attributes', 'NNS'), ('or', 'CC'), ('columns', 'NNS'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'DT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "\n",
    "\n",
    "sentences=sent_tokenize(data)\n",
    "\n",
    "list=[]\n",
    "for sentence in sentences:\n",
    "    list.append(word_tokenize(sentence))\n",
    "    \n",
    "print(nltk.pos_tag_sents(list))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "UnigramTagger permet d'attribuer aux mots leurs tags les plus frequents par rapport à un corpus d'apprentissage.    finds the most likely tag for each word in a training corpus, and then uses that information to assign tags to new tokens.corpus Ensemble fini de textes choisi comme base d'une étude.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to /home/etd/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8121200039868434\n",
      "[('Big', 'JJ-TL'), ('data', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('field', 'NN'), ('that', 'CS'), ('treats', None), ('ways', 'NNS'), ('to', 'TO'), ('analyze', None), (',', ','), ('systematically', None), ('extract', None), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'VB'), ('with', 'IN'), ('data', 'NN'), ('sets', 'VBZ-HL'), ('that', 'CS'), ('are', 'BER'), ('too', 'QL'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'JJ'), ('to', 'TO'), ('be', 'BE'), ('dealt', 'VBD'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', None), ('application', 'NN'), ('software', None), ('.', '.'), ('Data', None), ('with', 'IN'), ('many', 'AP'), ('cases', 'NNS'), ('(', '('), ('rows', None), (')', ')'), ('offer', 'VB'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'CS'), ('data', 'NN'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'AP'), ('attributes', None), ('or', 'CC'), ('columns', None), (')', ')'), ('may', 'MD'), ('lead', 'NN'), ('to', 'TO'), ('a', 'AT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', None), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('brown')\n",
    "from nltk.corpus import brown #The Brown Corpus was the first million-word electronic corpus of English\n",
    "from nltk.tag import UnigramTagger\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "unigram_tagger = nltk.UnigramTagger(train_sents)\n",
    "print(unigram_tagger.evaluate(test_sents))\n",
    "\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(unigram_tagger.tag(word_tokenize(data)))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "le modèle n-gram est une generalisation de l'unigram qui considère également le contexte où apparait le mot en considerant les tags des n-1 mots precedents.\n",
    "\n",
    "bigram tagger est un exemple generateur pos-tagging n-gram. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10206319146815508\n",
      "[('big', None), ('data', None), ('is', None), ('a', None), ('field', None), ('that', None), ('treats', None), ('ways', None), ('to', None), ('analyze', None), (',', None), ('systematically', None), ('extract', None), ('information', None), ('from', None), (',', None), ('or', None), ('otherwise', None), ('deal', None), ('with', None), ('data', None), ('sets', None), ('that', None), ('are', None), ('too', None), ('large', None), ('or', None), ('complex', None), ('to', None), ('be', None), ('dealt', None), ('with', None), ('by', None), ('traditional', None), ('data-processing', None), ('application', None), ('software', None), ('.', None), ('data', None), ('with', None), ('many', None), ('cases', None), ('(', None), ('rows', None), (')', None), ('offer', None), ('greater', None), ('statistical', None), ('power', None), (',', None), ('while', None), ('data', None), ('with', None), ('higher', None), ('complexity', None), ('(', None), ('more', None), ('attributes', None), ('or', None), ('columns', None), (')', None), ('may', None), ('lead', None), ('to', None), ('a', None), ('higher', None), ('false', None), ('discovery', None), ('rate', None), ('.', None)]\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "bigram_tagger = nltk.BigramTagger(train_sents)\n",
    "print(bigram_tagger.evaluate(test_sents))\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(bigram_tagger.tag(word_tokenize(data.lower()))) #bigram= on traite deux termes par deux"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "combining taggers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8452108043456593\n",
      "[('Big', 'JJ-TL'), ('data', 'NN'), ('is', 'BEZ'), ('a', 'AT'), ('field', 'NN'), ('that', 'CS'), ('treats', 'NN'), ('ways', 'NNS'), ('to', 'TO'), ('analyze', 'NN'), (',', ','), ('systematically', 'NN'), ('extract', 'NN'), ('information', 'NN'), ('from', 'IN'), (',', ','), ('or', 'CC'), ('otherwise', 'RB'), ('deal', 'VB'), ('with', 'IN'), ('data', 'NN'), ('sets', 'VBZ-HL'), ('that', 'CS'), ('are', 'BER'), ('too', 'QL'), ('large', 'JJ'), ('or', 'CC'), ('complex', 'NN'), ('to', 'TO'), ('be', 'BE'), ('dealt', 'VBD'), ('with', 'IN'), ('by', 'IN'), ('traditional', 'JJ'), ('data-processing', 'NN'), ('application', 'NN'), ('software', 'NN'), ('.', '.'), ('Data', 'NN'), ('with', 'IN'), ('many', 'AP'), ('cases', 'NNS'), ('(', '('), ('rows', 'NN'), (')', ')'), ('offer', 'VB'), ('greater', 'JJR'), ('statistical', 'JJ'), ('power', 'NN'), (',', ','), ('while', 'CS'), ('data', 'NN'), ('with', 'IN'), ('higher', 'JJR'), ('complexity', 'NN'), ('(', '('), ('more', 'AP'), ('attributes', 'NN'), ('or', 'CC'), ('columns', 'NN'), (')', ')'), ('may', 'MD'), ('lead', 'VB'), ('to', 'TO'), ('a', 'AT'), ('higher', 'JJR'), ('false', 'JJ'), ('discovery', 'NN'), ('rate', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "brown_tagged_sents = brown.tagged_sents(categories='news')\n",
    "\n",
    "size = int(len(brown_tagged_sents) * 0.9)\n",
    "train_sents = brown_tagged_sents[:size]\n",
    "test_sents = brown_tagged_sents[size:]\n",
    "t0 = nltk.DefaultTagger('NN')\n",
    "t1 = nltk.UnigramTagger(train_sents, backoff=t0)\n",
    "t2 = nltk.BigramTagger(train_sents, backoff=t1)\n",
    "print(t2.evaluate(test_sents))\n",
    "data=\"Big data is a field that treats ways to analyze, systematically extract information from,\" \\\n",
    "     \" or otherwise deal with data sets that are too large or complex to be dealt with by traditional\" \\\n",
    "     \" data-processing application software. Data with many cases (rows) offer greater statistical power,\" \\\n",
    "     \" while data with higher complexity (more attributes or columns) \" \\\n",
    "     \"may lead to a higher false discovery rate. \"\n",
    "print(t2.tag(word_tokenize(data)))\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour le moment la package nltk ne permet de faire le pos-tagging que pour l’anglais et le russe à l’aide du modèle « averaged_perceptron_tagger ». \n",
    "StanfordPOSTagguer permet faire du pos-tagging pour d’autre langues comme le français et l’arabe. Il suffit de télécharger les differents librairies nécessaires (https://nlp.stanford.edu/software/tagger.shtml) et utiliser celles qui correspondent à la langue comme présenté dans l’exemple ci-dessous. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.6.\tNER (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La fonction nltk.ne_chunk () permet de reconnaitre les entités d’une expression texte à l'aide du modèle « maxent_ne_chunker » qui est consacré au NER pour la langue anglaise. Pour d’autre langues spaCy est recommandé. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(S\n",
      "  (PERSON William/NNP)\n",
      "  (PERSON Henry/NNP Gates/NNP III/NNP)\n",
      "  (/(\n",
      "  born/JJ\n",
      "  October/NNP\n",
      "  28/CD\n",
      "  ,/,\n",
      "  1955/CD\n",
      "  )/)\n",
      "  is/VBZ\n",
      "  an/DT\n",
      "  (GPE American/JJ)\n",
      "  business/NN\n",
      "  magnate/NN\n",
      "  ,/,\n",
      "  software/NN\n",
      "  developer/NN\n",
      "  ,/,\n",
      "  and/CC\n",
      "  philanthropist/NN\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  best/RB\n",
      "  known/VBN\n",
      "  as/IN\n",
      "  the/DT\n",
      "  co-founder/NN\n",
      "  of/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  Corporation.During/VBG\n",
      "  his/PRP$\n",
      "  career/NN\n",
      "  at/IN\n",
      "  (ORGANIZATION Microsoft/NNP)\n",
      "  ,/,\n",
      "  (PERSON Gates/NNP)\n",
      "  held/VBD\n",
      "  the/DT\n",
      "  positions/NNS\n",
      "  of/IN\n",
      "  chairman/NN\n",
      "  ,/,\n",
      "  chief/JJ\n",
      "  executive/NN\n",
      "  officer/NN\n",
      "  (/(\n",
      "  (ORGANIZATION CEO/NNP)\n",
      "  )/)\n",
      "  ,/,\n",
      "  president/NN\n",
      "  and/CC\n",
      "  chief/NN\n",
      "  software/NN\n",
      "  architect/NN\n",
      "  ,/,\n",
      "  while/IN\n",
      "  also/RB\n",
      "  being/VBG\n",
      "  the/DT\n",
      "  largest/JJS\n",
      "  individual/JJ\n",
      "  shareholder/NN\n",
      "  until/IN\n",
      "  May/NNP\n",
      "  2014/CD\n",
      "  ./.\n",
      "  He/PRP\n",
      "  is/VBZ\n",
      "  one/CD\n",
      "  of/IN\n",
      "  the/DT\n",
      "  best-known/JJ\n",
      "  entrepreneurs/NNS\n",
      "  and/CC\n",
      "  pioneers/NNS\n",
      "  of/IN\n",
      "  the/DT\n",
      "  microcomputer/NN\n",
      "  revolution/NN\n",
      "  of/IN\n",
      "  the/DT\n",
      "  1970s/CD\n",
      "  and/CC\n",
      "  1980s/CD\n",
      "  ./.)\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "data=\"William Henry Gates III (born October 28, 1955) is an American business magnate,\" \\\n",
    "     \" software developer, and philanthropist. He is best known as the co-founder of Microsoft Corporation.\" \\\n",
    "     \"During his career at Microsoft, Gates held the positions of chairman, \" \\\n",
    "     \"chief executive officer (CEO), president and chief software architect, \" \\\n",
    "     \"while also being the largest individual shareholder until May 2014. \" \\\n",
    "     \"He is one of the best-known entrepreneurs and pioneers of the microcomputer \" \\\n",
    "     \"revolution of the 1970s and 1980s.\"\n",
    "words=word_tokenize(data)\n",
    "print(nltk.ne_chunk(nltk.pos_tag(words)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.7.\tWSD"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Un mot peut avoir plusieurs significations selon son contexte (les mots voisins et le rôle grammaticale). Par exemple, le mot anglais « break » possède 75 sens. Chose qui montre l’importance de la désambiguïsation lors de l’analyse d’un texte. Ci-dessous un extrait de la récupération des différents sens du mot « break » avec leurs annotations grammaticales. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">>> some abrupt occurrence that interrupts an ongoing activity\n",
      ">>> an unexpected piece of good luck\n",
      ">>> (geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n",
      ">>> a personal or social separation (as between opposing factions)\n",
      ">>> a pause from doing something (as work)\n",
      ">>> the act of breaking something\n",
      ">>> a time interval during which there is a temporary cessation of something\n",
      ">>> breaking of hard tissue such as bone\n",
      ">>> the occurrence of breaking\n",
      ">>> an abrupt change in the tone or register of the voice (as at puberty or due to emotion)\n",
      ">>> the opening shot that scatters the balls in billiards or pool\n",
      ">>> (tennis) a score consisting of winning a game when your opponent was serving\n",
      ">>> an act of delaying or interrupting the continuity\n",
      ">>> a sudden dash\n",
      ">>> any frame in which a bowler fails to make a strike or spare\n",
      ">>> an escape from jail\n",
      ">>> terminate\n",
      ">>> become separated into pieces or fragments\n",
      ">>> render inoperable or ineffective\n",
      ">>> ruin completely\n",
      ">>> destroy the integrity of; usually by force; cause to separate into pieces or fragments\n",
      ">>> act in disregard of laws, rules, contracts, or promises\n",
      ">>> move away or escape suddenly\n",
      ">>> scatter or part\n",
      ">>> force out or release suddenly and often violently something pent up\n",
      ">>> prevent completion\n",
      ">>> enter someone's (virtual or real) property in an unauthorized manner, usually with the intent to steal or commit a violent act\n",
      ">>> make submissive, obedient, or useful\n",
      ">>> fail to agree with; be in violation of; as of rules or patterns\n",
      ">>> surpass in excellence\n",
      ">>> make known to the public information that was previously known only to a few people or that was meant to be kept a secret\n",
      ">>> come into being\n",
      ">>> stop operating or functioning\n",
      ">>> interrupt a continued activity\n",
      ">>> make a rupture in the ranks of the enemy or one's own by quitting or fleeing\n",
      ">>> curl over and fall apart in surf or foam, of waves\n",
      ">>> lessen in force or effect\n",
      ">>> be broken in\n",
      ">>> come to an end\n",
      ">>> vary or interrupt a uniformity or continuity\n",
      ">>> cause to give up a habit\n",
      ">>> give up\n",
      ">>> come forth or begin from a state of latency\n",
      ">>> happen or take place\n",
      ">>> cause the failure or ruin of\n",
      ">>> invalidate by judicial action\n",
      ">>> discontinue an association or relation; go different ways\n",
      ">>> assign to a lower position; reduce in rank\n",
      ">>> reduce to bankruptcy\n",
      ">>> change directions suddenly\n",
      ">>> emerge from the surface of a body of water\n",
      ">>> break down, literally or metaphorically\n",
      ">>> do a break dance\n",
      ">>> exchange for smaller units of money\n",
      ">>> destroy the completeness of a set of related items\n",
      ">>> make the opening shot that scatters the balls\n",
      ">>> separate from a clinch, in boxing\n",
      ">>> go to pieces\n",
      ">>> break a piece from a whole\n",
      ">>> become punctured or penetrated\n",
      ">>> pierce or penetrate\n",
      ">>> be released or become known; of news\n",
      ">>> cease an action temporarily\n",
      ">>> interrupt the flow of current in\n",
      ">>> undergo breaking\n",
      ">>> find a flaw in\n",
      ">>> find the solution or key to\n",
      ">>> change suddenly from one tone quality or register to another\n",
      ">>> happen\n",
      ">>> become fractured; break or crack on the surface only\n",
      ">>> crack; of the male voice in puberty\n",
      ">>> fall sharply\n",
      ">>> fracture a bone of\n",
      ">>> diminish or discontinue abruptly\n",
      ">>> weaken or destroy in spirit or body\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "for sens in wordnet.synsets('break'):\n",
    "    print(\">>>\",sens.definition())\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La bibliothèque nltk offre à travers le module wsd la possibilité de détecter le sens d’un mot en fonction de son contexte. A cette fin, l’algorithme Lesk est utilisé pour réaliser une désambiguïsation du sens d’un mot en retournant le sens qui a permis d’avoir le plus grand nombre de termes en intersection avec le contexte du mot pour lequel on est en train de chercher le sens exact. L’algorithme ne retourne aucun sens s’il n’arrive pas à réaliser la désambiguïsation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(geology) a crack in the earth's crust resulting from the displacement of one side with respect to the other\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "context= word_tokenize(\"I've just finished the first step of the competition. I need a little break to catch my breath\")\n",
    "print(lesk(context, 'break','n').definition())"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "L’exemple ci-dessus montre que l’algorithme n’est pas assez performant. D’autres algorithmes peuvent être utilisés en se basant sur les bibliothèques baseline, pywsd ou spaCy. Ci-dessous un autre exemple avec la bibliotheque pywsd.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset('rupture.n.02')\n",
      "a personal or social separation (as between opposing factions)\n"
     ]
    }
   ],
   "source": [
    "# pip install pywsd\n",
    "# pip install -U pywsd\n",
    "#pip install wn==0.0.22\n",
    "from pywsd.lesk import simple_lesk\n",
    "sent = \"I've just finished the first step of the competition. I need a little break to catch my breath\"\n",
    "ambiguous = 'break'\n",
    "answer = simple_lesk(sent, ambiguous, pos='n')\n",
    "print (answer)\n",
    "print (answer.definition())\n",
    "#It is based on the hypothesis that words used together \n",
    "#in text are related to each other and that the relation \n",
    "#can be observed in the definitions of the words and their senses.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.8.\tSpacy"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "spaCy une bibliothèque de la NLP qui est très puissante (elle est orientée production, non uniquement pour la recherche ou l’apprentissage de la NLP), totalement écrite python, gratuite et libre.\n",
    "Par défaut, lorsqu’on fait appel au module nlp de spaCy, les opérations suivantes sont exécutées : Segmentation, pos-tagging, analyse syntaxique, NER (Named Entity Recognition), etc.  Un objet Doc est retourné à l’issue de toutes les opérations et qui encapsule tous les resultats de l’analyse. \n",
    "\n",
    "L’exemple ci-dessous montre comment extraire les différentes informations à partir d’un objet Doc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/token: Le /lemma: le Xx True True /POS: DET /PARS: big det /NER: O \n",
      "/token: big /lemma: big xxx True False /POS: NUM /PARS: data nummod /NER: O \n",
      "/token: data /lemma: data xxxx True False /POS: ADP /PARS: data ROOT /NER: O \n",
      "/token:   /lemma:     False False /POS: SPACE /PARS: data punct /NER: O \n",
      "/token: « /lemma: « « False False /POS: NOUN /PARS: « ROOT /NER: O \n",
      "/token: grosses /lemma: gros xxxx True False /POS: ADJ /PARS: données amod /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: données ROOT /NER: O \n",
      "/token: » /lemma: » » False False /POS: PUNCT /PARS: données punct /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: anglais case /NER: O \n",
      "/token: anglais /lemma: anglais xxxx True False /POS: NOUN /PARS: données obl:mod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: mégadonnées det /NER: O \n",
      "/token: mégadonnées /lemma: mégadonnée xxxx True False /POS: NOUN /PARS: désigne nsubj /NER: O \n",
      "/token: ou /lemma: ou xx True True /POS: CCONJ /PARS: données cc /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: données det /NER: O \n",
      "/token: données /lemma: donnée xxxx True False /POS: NOUN /PARS: mégadonnées conj /NER: O \n",
      "/token: massives /lemma: massif xxxx True False /POS: ADJ /PARS: données amod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: désigne /lemma: désigne xxxx True False /POS: VERB /PARS: désigne ROOT /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: ressources det /NER: O \n",
      "/token: ressources /lemma: ressource xxxx True False /POS: NOUN /PARS: désigne obj /NER: O \n",
      "/token: d’ /lemma: d’ x’ False True /POS: NOUN /PARS: ressources acl /NER: B PER\n",
      "/token: informations /lemma: information xxxx True False /POS: NOUN /PARS: ressources nmod /NER: O \n",
      "/token: dont /lemma: dont xxxx True True /POS: PRON /PARS: caractéristiques nmod /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: caractéristiques det /NER: O \n",
      "/token: caractéristiques /lemma: caractéristique xxxx True False /POS: NOUN /PARS: informations acl:relcl /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: termes case /NER: O \n",
      "/token: termes /lemma: terme xxxx True False /POS: NOUN /PARS: caractéristiques nmod /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: volume case /NER: O \n",
      "/token: volume /lemma: volume xxxx True False /POS: NOUN /PARS: termes nmod /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: vélocité punct /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: vélocité case /NER: O \n",
      "/token: vélocité /lemma: vélocité xxxx True False /POS: NOUN /PARS: termes conj /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: variété cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: variété case /NER: O \n",
      "/token: variété /lemma: variété xxxx True False /POS: NOUN /PARS: termes conj /NER: O \n",
      "/token: imposent /lemma: imposent xxxx True False /POS: ADV /PARS: variété advmod /NER: O \n",
      "/token: l’ /lemma: l’ x’ False True /POS: ADJ /PARS: utilisation case /NER: O \n",
      "/token: utilisation /lemma: utilisation xxxx True False /POS: NOUN /PARS: variété obj /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: technologies case /NER: O \n",
      "/token: technologies /lemma: technologie xxxx True False /POS: NOUN /PARS: utilisation nmod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: méthodes cc /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: méthodes case /NER: O \n",
      "/token: méthodes /lemma: méthode xxxx True False /POS: NOUN /PARS: technologies conj /NER: O \n",
      "/token: analytiques /lemma: analytique xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: particulières /lemma: particulier xxxx True False /POS: ADJ /PARS: méthodes amod /NER: O \n",
      "/token: pour /lemma: pour xxxx True True /POS: ADP /PARS: générer mark /NER: O \n",
      "/token: générer /lemma: générer xxxx True False /POS: VERB /PARS: variété advcl /NER: O \n",
      "/token: de /lemma: de xx True True /POS: ADP /PARS: valeur case /NER: O \n",
      "/token: la /lemma: le xx True True /POS: DET /PARS: valeur det /NER: O \n",
      "/token: valeur /lemma: valeur xxxx True False /POS: NOUN /PARS: générer obl:arg /NER: O \n",
      "/token: , /lemma: , , False False /POS: PUNCT /PARS: désigne punct /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: dépassent cc /NER: O \n",
      "/token: qui /lemma: qui xxx True True /POS: PRON /PARS: dépassent nsubj /NER: O \n",
      "/token: dépassent /lemma: dépasser xxxx True False /POS: VERB /PARS: désigne conj /NER: O \n",
      "/token: en /lemma: en xx True True /POS: ADP /PARS: général case /NER: O \n",
      "/token: général /lemma: général xxxx True False /POS: NOUN /PARS: dépassent obl:arg /NER: O \n",
      "/token: les /lemma: le xxx True True /POS: DET /PARS: capacités det /NER: O \n",
      "/token: capacités /lemma: capacité xxxx True False /POS: NOUN /PARS: dépassent obj /NER: O \n",
      "/token: d' /lemma: de x' False True /POS: ADP /PARS: machine case /NER: O \n",
      "/token: une /lemma: un xxx True True /POS: DET /PARS: machine det /NER: O \n",
      "/token: seule /lemma: seul xxxx True True /POS: ADJ /PARS: machine amod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: unique cc /NER: O \n",
      "/token: unique /lemma: unique xxxx True False /POS: ADJ /PARS: seule conj /NER: O \n",
      "/token: machine /lemma: machine xxxx True False /POS: NOUN /PARS: capacités nmod /NER: O \n",
      "/token: et /lemma: et xx True True /POS: CCONJ /PARS: nécessitent cc /NER: O \n",
      "/token: nécessitent /lemma: nécessiter xxxx True False /POS: VERB /PARS: dépassent conj /NER: O \n",
      "/token: des /lemma: un xxx True True /POS: DET /PARS: traitements det /NER: O \n",
      "/token: traitements /lemma: traitement xxxx True False /POS: NOUN /PARS: nécessitent obj /NER: O \n",
      "/token: parallélisés /lemma: paralléliser xxxx True False /POS: VERB /PARS: traitements acl /NER: O \n"
     ]
    }
   ],
   "source": [
    "#!pip install spacy\n",
    "import spacy\n",
    "nlp = spacy.load('fr_core_news_sm')\n",
    "doc = nlp(\"Le big data  « grosses données » en anglais,les mégadonnées ou les données massives, \" \\\n",
    "     \"désigne les ressources d’informations dont les caractéristiques en termes de volume,\" \\\n",
    "     \" de vélocité et de variété imposent l’utilisation de technologies et de méthodes analytiques \" \\\n",
    "     \"particulières pour générer de la valeur, et qui dépassent en général les capacités \" \\\n",
    "     \"d'une seule et unique machine et nécessitent des traitements parallélisés\")\n",
    "\n",
    "for token in doc:\n",
    "    print(\"/token:\",token.text, \"/lemma:\",token.lemma_, token.shape_, token.is_alpha, token.is_stop,\"/POS:\", token.tag_, \"/PARS:\", token.head, token.dep_, \"/NER:\", token.ent_iob_, token.ent_type_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.\tModel représentatif d’un document"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Afin de simplifier l’analyse des données texte, il est recommandé d’utiliser des représentations plus consistantes qu’une simple segmentation. Il faut bien évidement réaliser des prétraitements tels que la racinisation, la lemmatisation, supprimer les redondances, supprimer les mots qui représentent le même sens. Mais c’est encore insuffisant pour obtenir un modèle représentatif qui reflète l’importance et le sens exacte de chaque mot dans une expression ou dans un document texte.\n",
    "Afin de repondre à ce besoin, plusieurs représentations vectorielles des termes contenus dans un texte sont possibles : one-hot-vector, Bag-of-words, TF-IDF et Word2vec...t. Nous utilisant scikitlearn pour réaliser ces différentes représentations vectorielles. Ça n’empêche pas que ces représentations vectorielles peuvent être obtenues en faisant du codage from scratch. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2. One-hot-vector"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et determine par la suite pour chaque document/expression la présence de chaque terme du vocabulaire. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 1 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "from sklearn.preprocessing import Binarizer\n",
    "\n",
    "freq   = CountVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "corpus = freq.fit_transform(corpus)\n",
    "#print(corpus.toarray())\n",
    "onehot = Binarizer()\n",
    "corpus = onehot.fit_transform(corpus.toarray())\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1.\tBag-of-words"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la représentation vectorielle Bag-of-Words, le modèle commence par la création d’un vocabulaire à partir du corpus formé par tous les documents ou les expressions texte et calcul par la suite pour chaque document/expression le nombre d’occurrences de chaque terme du vocabulaire. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1 1 1 0 0 1 0 1]\n",
      " [0 1 0 1 0 2 1 0 1]\n",
      " [1 0 0 0 1 0 1 1 0]\n",
      " [0 1 1 1 0 0 1 0 1]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, 0, 0, 0, 0, 0, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "X = vectorizer.fit_transform(corpus) #sparsy format\n",
    "print(X.toarray()) # explicit matrix format\n",
    "print(vectorizer.get_feature_names() ) #vocabulary as list of string\n",
    "vectorizer.vocabulary_.get('document') #get column index of a specific term in the vocabulary\n",
    "vectorizer.transform(['Something completely new.']).toarray()#apply the model to a new document"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2.\tTF-IDF"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "La représentation vectorielle Bag-of-Words considère la fréquence d’apparition des termes du vocabulaire dans chaque document du corpus séparément des autres. Cette représentation néglige l’importance du terme par rapport au corpus tout en entier.  TF-IDF (term-frequency times inverse document-frequency) est un autre modèle de représentation vectorielle des occurrences des termes d’un document en considérant également leurs occurrences dans tout le corpus. Cette approche va permettre de diminuer l’importance des termes les plus fréquents dans des documents texte tels que les stop words.\n",
    "How? The highest scoring words of a document are the most relevant to that document, and therefore they can be considered keywords for that document\n",
    "\n",
    "tf-idf(t,d)=tf(t,d)×idf(t).\n",
    "\n",
    "idf(t)=log[(1+n)/(1+df(t))]+1\n",
    "n: la taille du corpus.\n",
    "df(t) :  le nombre de documents qui comportent le terme t."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]\n",
      " [0.         0.27230147 0.         0.27230147 0.         0.85322574\n",
      "  0.22262429 0.         0.27230147]\n",
      " [0.55280532 0.         0.         0.         0.55280532 0.\n",
      "  0.28847675 0.55280532 0.        ]\n",
      " [0.         0.43877674 0.54197657 0.43877674 0.         0.\n",
      "  0.35872874 0.         0.43877674]]\n",
      "['and', 'document', 'first', 'is', 'one', 'second', 'the', 'third', 'this']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "corpus = ['This is the first document.','this is the second second Document.','And the third one.','Is this the first document?']\n",
    "vectorizer = TfidfVectorizer()\n",
    "X=vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())\n",
    "print(vectorizer.get_feature_names() )\n",
    "\n",
    "#NB: le vecteur tf-idf obtenu sera normalisé pour obtenir des valeurs entre 0 et 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.\tCas utilisations : Détection du Plagiarisme "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L’objectif de cet atelier est de détecter le pélagianisme à partir de wikipedia pendant la préparation des réponses à un certain nombre de questions sur des connaissances en informatique. Le dataset utilisé peut-être récupéré à partir du lien suivant :Cliquer <a href=\"https://ir.shef.ac.uk/cloughie/resources/plagiarism_corpus.html#Download\" target=\"_blank\">ICI</a> \n",
    "\n",
    "Pour ce faire, nous nous basant sur le calcul des similarités entre les réponses des candidats et les définitions exactes trouvées sur Wikipédia. Deux méthodes de calcul de similarité sont à utiliser, à savoir, la similarité syntaxique et la similarité sémantique."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recuperer le dataset\n",
    "**Réaliser les différentes tâches de prétraitement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "files=glob.glob('/home/etd/Desktop/M12/TM/TM/corpus-20090418/*')\n",
    "f_pointers=[open(file,\"r\",encoding=\"utf8\",errors='ignore') for file in sorted(files)]\n",
    "corpus=[f.read() for f in f_pointers]\n",
    "#print(corpus)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = [nlp(doc) for doc in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.en.stop_words import STOP_WORDS\n",
    "def preprocess(doc_nlp):\n",
    "    d=[]\n",
    "    for token in doc_nlp:\n",
    "        if(not token.text in STOP_WORDS and  token.text.isalpha()):\n",
    "            d.append(token.lemma_)\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['inheritance',\n",
       " 'basic',\n",
       " 'concept',\n",
       " 'Object',\n",
       " 'orient',\n",
       " 'programming',\n",
       " 'basic',\n",
       " 'idea',\n",
       " 'create',\n",
       " 'new',\n",
       " 'class',\n",
       " 'add',\n",
       " 'extra',\n",
       " 'detail',\n",
       " 'exist',\n",
       " 'class',\n",
       " 'this',\n",
       " 'allow',\n",
       " 'new',\n",
       " 'class',\n",
       " 'reuse',\n",
       " 'method',\n",
       " 'variable',\n",
       " 'exist',\n",
       " 'class',\n",
       " 'new',\n",
       " 'method',\n",
       " 'class',\n",
       " 'add',\n",
       " 'specialise',\n",
       " 'new',\n",
       " 'class',\n",
       " 'inheritance',\n",
       " 'model',\n",
       " 'kind',\n",
       " 'relationship',\n",
       " 'entity',\n",
       " 'object',\n",
       " 'example',\n",
       " 'postgraduate',\n",
       " 'undergraduate',\n",
       " 'kind',\n",
       " 'student',\n",
       " 'this',\n",
       " 'kind',\n",
       " 'relationship',\n",
       " 'visualise',\n",
       " 'tree',\n",
       " 'structure',\n",
       " 'student',\n",
       " 'general',\n",
       " 'root',\n",
       " 'node',\n",
       " 'postgraduate',\n",
       " 'undergraduate',\n",
       " 'specialised',\n",
       " 'extension',\n",
       " 'student',\n",
       " 'node',\n",
       " 'child',\n",
       " 'node',\n",
       " 'in',\n",
       " 'relationship',\n",
       " 'student',\n",
       " 'know',\n",
       " 'superclass',\n",
       " 'parent',\n",
       " 'class',\n",
       " 'postgraduate',\n",
       " 'know',\n",
       " 'subclass',\n",
       " 'child',\n",
       " 'class',\n",
       " 'postgraduate',\n",
       " 'class',\n",
       " 'extend',\n",
       " 'student',\n",
       " 'class',\n",
       " 'inheritance',\n",
       " 'occur',\n",
       " 'layer',\n",
       " 'visualise',\n",
       " 'display',\n",
       " 'large',\n",
       " 'tree',\n",
       " 'structure',\n",
       " 'for',\n",
       " 'example',\n",
       " 'extend',\n",
       " 'postgraduate',\n",
       " 'node',\n",
       " 'add',\n",
       " 'extra',\n",
       " 'extended',\n",
       " 'class',\n",
       " 'call',\n",
       " 'MSc',\n",
       " 'Student',\n",
       " 'phd',\n",
       " 'student',\n",
       " 'type',\n",
       " 'student',\n",
       " 'kind',\n",
       " 'postgraduate',\n",
       " 'student',\n",
       " 'this',\n",
       " 'mean',\n",
       " 'MSc',\n",
       " 'student',\n",
       " 'phd',\n",
       " 'student',\n",
       " 'class',\n",
       " 'inherit',\n",
       " 'method',\n",
       " 'variable',\n",
       " 'postgraduate',\n",
       " 'student',\n",
       " 'class']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(doc[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On aura besoin que de la lemmatization aprés avoir rendu le doc sous forme de token pour pouvoir faire la similarité syntaxique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.1 similarité syntaxique "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la similarité syntaxique entre des vecteurs, plusieurs distances sont possibles à savoir : distance, euclidienne, Cosine Jaccard, Levenshtein, Hamming…\n",
    "\n",
    "l'exemple ci-dessous permet de calculer la similarité. semantique entre les documents en se basant sur une representation vectorielle en TFIDF avec la distance euclidienne. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 1.05990529, 1.33904078, 0.        ]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "tfidf_matrix.shape\n",
    "\n",
    "#compute similarity for first sentence with rest of the sentences\n",
    "euclidean_distances(tfidf_matrix[0],tfidf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Realiser la même chose en se basant sur les representations OHV et BOW?\n",
    "**Comparer les performances des trois methodes en terme de temps d'execution et precision.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OHV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.23606798, 2.64575131, 0.        ]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "freq   = CountVectorizer()\n",
    "c=[sent.lower() for sent in corpus]\n",
    "cor = freq.fit_transform(corpus)\n",
    "\n",
    "#compute similarity for first sentence with rest of the sentences\n",
    "euclidean_distances(cor[0],cor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.        , 2.23606798, 2.64575131, 0.        ]])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "corpus = ['This is the first document.','This is the second second document.','And the third one.','Is this the first document?']\n",
    "corpus=[ sent.lower() for sent in corpus]\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "euclidean_distances(X[0],X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "à mon avis le plus précis est le tfidf car il prend en considération meme la fréquence du terme dans\n",
    "tous le corpus c'est ce qui fait de lui plus précis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe d'autre methodes(orientées caractères) pour le calcule de la similarité syntaxique entre des documents courts (des phrases):\n",
    "\n",
    "* Longest Common Sequence (LCS)\n",
    "* Set features\n",
    "* Word Order Similarity\n",
    "* n-gram sentences \n",
    "* Jaro-Winkler\n",
    "* ...\n",
    "\n",
    "Cliquer <a href=\"http://www.iaeme.com/MasterAdmin/Journal_uploads/IJCET/VOLUME_9_ISSUE_5/IJCET_09_05_001.pdf\" target=\"_blank\">ICI</a> pour plus de details\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## en utilisant la distance euclidienne on a:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Calculer les similarités syntaxiques entre les réponses des étudiants et les définitions trouvées sur wikipedia.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_distances,euclidean_distances\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "import time\n",
    "\n",
    "    \n",
    "def tfidf_simil(corpus,index_of_doc):\n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(corpus)\n",
    "    #tfidf_matrix = tfidf_matrix.toarray()\n",
    "    #compute similarity for first sentence with rest of the sentences\n",
    "    return (euclidean_distances(tfidf_matrix[index_of_doc],tfidf_matrix))\n",
    "\n",
    "def ohv_simil(corpus,index_of_doc):\n",
    "    vectorizer  = CountVectorizer()\n",
    "    corpus=[ sent.lower() for sent in corpus]\n",
    "    corpus = vectorizer.fit_transform(corpus)\n",
    "\n",
    "    onehot = Binarizer()\n",
    "    ohv = onehot.fit_transform(corpus.toarray())\n",
    "\n",
    "    return (euclidean_distances(ohv[index_of_doc],ohv))\n",
    "\n",
    "def bow_simil(corpus,index_of_doc):\n",
    "    vectorizer = CountVectorizer(lowercase=True)\n",
    "    corpus=[ sent.lower() for sent in corpus]\n",
    "    X = vectorizer.fit_transform(corpus) #sparsy format\n",
    "    bow = X.toarray()\n",
    "    \n",
    "    return (euclidean_distances(bow[index_of_doc],bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "task1=corpus[::5]\n",
    "task2=corpus[2::5]\n",
    "task3=corpus[3::5]\n",
    "task4=corpus[4::5]\n",
    "task5=corpus[5::5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In object-oriented programming, inheritance is a way to form new classes (instances of which are called objects) using classes that have already been defined. The inheritance concept was invented in 1967 for Simula.\\n\\nThe new classes, known as derived classes, take over (or inherit) attributes and behavior of the pre-existing classes, which are referred to as base classes (or ancestor classes). It is intended to help reuse existing code with little or no modification.\\n\\nInheritance provides the support for representation by categorization in computer languages. Categorization is a powerful mechanism number of information processing, crucial to human learning by means of generalization (what is known about specific entities is applied to a wider group given a belongs relation can be established) and cognitive economy (less information needs to be stored about each specific entity, only its particularities).\\n\\nInheritance is also sometimes called generalization, because the is-a relationships represent a hierarchy between classes of objects. For instance, a \"fruit\" is a generalization of \"apple\", \"orange\", \"mango\" and many others. One can consider fruit to be an abstraction of apple, orange, etc. Conversely, since apples are fruit (i.e., an apple is-a fruit), apples may naturally inherit all the properties common to all fruit, such as being a fleshy container for the seed of a plant.\\n\\nAn advantage of inheritance is that modules with sufficiently similar interfaces can share a lot of code, reducing the complexity of the program. Inheritance therefore has another view, a dual, called polymorphism, which describes many pieces of code being controlled by shared control code.\\nInheritance is typically accomplished either by overriding (replacing) one or more methods exposed by ancestor, or by adding new methods to those exposed by an ancestor.\\n\\nComplex inheritance, or inheritance used within a design that is not sufficiently mature, may lead to the Yo-yo problem.'"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task1[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "similarité task A\n",
      "[[1.21604975 1.24146323 0.98659847 0.57204362 0.26714618 1.10777576\n",
      "  1.19542583 0.8764334  1.23305301 1.20174758 0.81033754 0.56792284\n",
      "  1.08550258 1.17308256 0.68003071 1.19385364 0.19780116 0.85570628\n",
      "  1.1399773  0.        ]]\n",
      "similarité task B\n",
      "[[0.4776493  0.35426795 1.05784689 0.67935839 1.08044227 0.86041962\n",
      "  0.68598582 1.01218865 0.39738727 0.93386414 0.89639488 1.24359115\n",
      "  0.5570358  0.87790326 1.18682773 0.75142571 1.16811934 1.06843693\n",
      "  0.87737995 0.        ]]\n",
      "similarité task C\n",
      "[[0.99924565 0.75573344 0.53520624 1.28972854 1.29019698 0.8226406\n",
      "  0.87719586 1.13034252 0.94724998 0.49532997 0.84346984 1.01950504\n",
      "  0.10291656 1.03004416 0.87150202 0.62752808 0.23014385 1.00695851\n",
      "  1.16894471 0.        ]]\n",
      "similarité task D\n",
      "[[1.12133015 0.77006205 0.88812801 1.24260994 0.95933764 1.10916744\n",
      "  0.8709961  0.95297741 1.00815952 0.60024941 0.92884517 1.03389619\n",
      "  1.05837247 0.79857418 0.99741824 0.4744536  0.68620107 0.75993597\n",
      "  1.09606108 0.        ]]\n",
      "similarité task E\n",
      "[[1.23947899 0.98261422 0.56817665 0.26538742 1.1046094  1.19294361\n",
      "  0.8724508  1.23160903 1.20169558 0.80608977 0.56732836 1.085171\n",
      "  1.17265519 0.67603488 1.19130291 0.19793889 0.85511519 1.1420594\n",
      "  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"similarité task A\")\n",
    "print(tfidf_simil(task1,-1))\n",
    "print(\"similarité task B\")\n",
    "print(tfidf_simil(task2,-1))\n",
    "print(\"simil\n",
    " 25,\n",
    " 26,\n",
    " 27,\n",
    " 28,\n",
    " 29,\n",
    " 30,arité task C\")\n",
    "print(tfidf_simil(task3,-1))\n",
    "print(\"similarité task D\")\n",
    "print(tfidf_simil(task4,-1))\n",
    "print(\"similarité task E\")\n",
    "print(tfidf_simil(task5,-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5.2 similarité semantique : Approche lexicale"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "WordNet est une base de données lexicale qui compoerte des concepts (termes) classifiés et reliés les uns aux autres à travers des realtions semantiques\n",
    "\n",
    "La composante principale de wordNet est le synset (synonym set) tel que chacun contient plusieurs mots qui partagent le même sens (des lemmas). Egalement, un mot peut appartenir à plusieurs synsets à la foix.\n",
    "\n",
    "l'exemple suivant montre comment recuperer les synsets d'aun mots et comment recuperer ses synonymes pour un sens particuliers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computer sens in wordNet:\n",
      " \t Sens : 0\n",
      " \t\t Sens definition: a machine for performing calculations automatically\n",
      "\t\t Lemmas for sense :['computer', 'computing_machine', 'computing_device', 'data_processor', 'electronic_computer', 'information_processing_system']\n",
      " \t Sens : 1\n",
      " \t\t Sens definition: an expert at calculation (or at operating calculating machines)\n",
      "\t\t Lemmas for sense :['calculator', 'reckoner', 'figurer', 'estimator', 'computer']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "computer_synsets = wn.synsets(\"computer\") \n",
    "print(\"Computer sens in wordNet:\")\n",
    "i=0;\n",
    "for sense in computer_synsets: \n",
    "    print(\" \\t Sens :\", i)\n",
    "    print(\" \\t\\t Sens definition: \"+sense.definition())\n",
    "    lemmas = [l.name() for l in sense.lemmas()]\n",
    "    print(\"\\t\\t Lemmas for sense :\" +str(lemmas))\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour la similarité sémantique, la bibliothèque nltk et à travers le module wordnet permet de mesurer la distance ou la similarité sémantique entre les sens des mots. Ainsi, en récupérant les sens synset1 et synset2 de deux mots quelconques plusieurs façons sont possibles pour calculer leur similarité\n",
    "•\tsynset1.path_similarity(synset2) : retourne leur ordre de similarité sous forme d’une valeur numérique entre 0 et 1 en se basant sur le plus court chemin qui relie les deux sens dans l’arborescence de wordnet.\n",
    "•\tsynset1.lch_similarity(synset2): qui se base sur l’algorithme Leacock-Chodorow\n",
    "•\tSynset1.wup_similarity(synset2): qui se base sur l’algorithme Wu-Palmer\n",
    "•\tsynset1.res_similarity(synset2, ic): qui se base sur l’algorithme Resnik:\n",
    "•\tsynset1.jcn_similarity(synset2, ic): qui se base sur l’algorithme Jiang-Conrath \n",
    "•\tsynset1.lin_similarity(synset2, ic): qui se base sur l’algorithme Lin \n",
    "\n",
    "\n",
    "l'exemple suivant montre comment calculer les similarité entres les sens des termes computer et device en se basant sur les metriques Leacock-Chodorow et Wu-Palmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Synset('survey.n.01'), Synset('study.n.02'), Synset('report.n.01'), Synset('study.n.04'), Synset('study.n.05'), Synset('discipline.n.01'), Synset('sketch.n.01'), Synset('cogitation.n.02'), Synset('study.n.09'), Synset('study.n.10'), Synset('analyze.v.01'), Synset('study.v.02'), Synset('study.v.03'), Synset('learn.v.04'), Synset('study.v.05'), Synset('study.v.06')]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>22</th>\n",
       "      <th>23</th>\n",
       "      <th>24</th>\n",
       "      <th>25</th>\n",
       "      <th>26</th>\n",
       "      <th>27</th>\n",
       "      <th>28</th>\n",
       "      <th>29</th>\n",
       "      <th>30</th>\n",
       "      <th>31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>lch</th>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.058824</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.055556</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>...</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.066667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wup</th>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.111111</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.555556</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.421053</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.444444</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.631579</td>\n",
       "      <td>0.153846</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           0         1         2         3         4         5         6   \\\n",
       "lch  0.055556  0.062500  0.066667  0.058824  0.111111  0.066667  0.111111   \n",
       "wup  0.105263  0.117647  0.125000  0.111111  0.555556  0.125000  0.555556   \n",
       "\n",
       "           7         8         9   ...        22        23        24  \\\n",
       "lch  0.055556  0.083333  0.066667  ...  0.090909  0.066667  0.166667   \n",
       "wup  0.105263  0.421053  0.125000  ...  0.444444  0.125000  0.631579   \n",
       "\n",
       "           25    26    27    28    29    30    31  \n",
       "lch  0.083333  None  None  None  None  None  None  \n",
       "wup  0.153846  None  None  None  None  None  None  \n",
       "\n",
       "[2 rows x 32 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import wordnet as wn \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "computer_synsets = wn.synsets(\"computer\") \n",
    "device_synsets = wn.synsets(\"study\") \n",
    "lch=[]\n",
    "wup=[]\n",
    "print(device_synsets)\n",
    "\n",
    "for s1 in computer_synsets:\n",
    "    for s2 in device_synsets:\n",
    "        lch.append(s1.path_similarity(s2))\n",
    "        wup.append(s1.wup_similarity(s2))\n",
    "\n",
    "pd.DataFrame([lch,wup],[\"lch\",\"wup\"])\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Souvent on aura besoin de recuperer les sens exactes des termes dans leurs contextes afin mesurer leurs similarité d'une manière plus precise "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a machine for performing calculations automatically\n",
      "a machine for performing calculations automatically\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "from nltk.tokenize import word_tokenize\n",
    "def WSD(word, doc):\n",
    "    context= word_tokenize(doc)\n",
    "    sens=lesk(context, word)\n",
    "    return sens\n",
    "\n",
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "\n",
    "\n",
    "print(WSD(\"Computer\", doc1).definition())\n",
    "print(WSD(\"Computer\", doc2).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=WSD(\"Computer\", doc1)\n",
    "b=WSD(\"science\", doc2)\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Pour comparer calculer la distance semantique entre deux documents, on aura besoin d'un"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def semanticD(doc1,doc2):\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    doc1_p = nlp(doc1)\n",
    "    doc2_p = nlp(doc2)\n",
    "    doc1_p=preprocess(doc1_p)\n",
    "    doc2_p=preprocess(doc2_p)\n",
    "    score=0\n",
    "    print(doc1_p)\n",
    "    print(doc2_p)\n",
    "    for tock1 in doc1_p:\n",
    "        for tock2 in doc2_p:\n",
    "            syn1 = WSD(tock1,doc1)\n",
    "            syn2 = WSD(tock2,doc2)\n",
    "            if syn1 is not None and syn2 is not None :\n",
    "                x=syn1.path_similarity(syn2)\n",
    "                if x is not None and x>0.25:\n",
    "                    sim.append(x)\n",
    "        if len(sim)!=0:\n",
    "            score+=max(sim)\n",
    "    return score/min(len(doc1_p),len(doc2_p))\n",
    "            \n",
    "                \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['I', 'm', 'pilot']\n",
      "['I', 'drive', 'airplane']\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "doc1='Computer science is the study of computers and computing concepts. It includes both hardware and software, as well as networking and the Internet'\n",
    "doc2='Computer science is the science that deals with the theory and methods of processing information in digital computers, the design of computer hardware and software, and the applications of computers.'\n",
    "#def SemanticDistanceDocs(doc1,doc2):\n",
    "doc1='I m not a pilot'\n",
    "doc2='I drive airplanes'\n",
    "\n",
    "#print(SemanticDistanceDocs(doc1,doc2))\n",
    "print(semanticD(doc1,doc2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a machine for performing calculations automatically\n",
      "a machine for performing calculations automatically\n"
     ]
    }
   ],
   "source": [
    "syn1 = WSD('pilote',doc1)\n",
    "syn2 = WSD('pilote',doc2)\n",
    "print(WSD(\"Computer\", doc1).definition())\n",
    "print(WSD(\"Computer\", doc2).definition())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.07142857142857142"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=a.path_similarity(b)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Defnir la fonction SemanticDistanceDocs(doc1,doc2) qui permet de calculer la distance semantique totale entre deux documents texte"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
